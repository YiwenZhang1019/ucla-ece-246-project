{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "from utils import mnist_reader\n",
    "from utils.data_load import load\n",
    "import codes\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section #1.1: Linear Regression\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from disk and perform preprocessing to prepare it for the linear regression problem.   \n",
    "    \"\"\"\n",
    "    X_train, y_train = load('./data/regression/regression_train.csv')\n",
    "    X_test, y_test = load('./data/regression/regression_test.csv')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test= get_data()  \n",
    "\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (a): \n",
    "## Plot the training and test data ##\n",
    "\n",
    "plt.plot(X_train, y_train,'o', color='black')\n",
    "plt.plot(X_test, y_test,'o', color='blue')\n",
    "plt.xlabel('Train data')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression\n",
    "In the following cells, you will build a linear regression. You will implement its loss function, then subsequently train it with gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the opimal solution using closed form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.Regression import Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (d): \n",
    "## Complete loss_and_grad function in Regression.py file and test your results.\n",
    "regression = Regression(m=1, reg_param=0)\n",
    "loss, grad = regression.loss_and_grad(X_train,y_train)\n",
    "print('Loss value',loss)\n",
    "print('Gradient value',grad)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (e): \n",
    "## Complete train_LR function in Regression.py file \n",
    "loss_history, w = regression.train_LR(X_train,y_train, eta=1e-3,batch_size=30, num_iters=10000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "print(w)\n",
    "print('Final loss:',loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (e) (Different Batch Sizes):\n",
    "from numpy.linalg import norm \n",
    "Batch = [1, 5, 15, 20, 25, 30]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the Logistic regression for different batch size Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(Batch,test_err)\n",
    "plt.xlabel('Batch_size')\n",
    "plt.ylabel('Test_error')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_Batch_test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (f): \n",
    "## Complete closed_form function in Regression.py file\n",
    "loss_2, w_2 = regression.closed_form(X_train, y_train)\n",
    "print('Optimal solution loss',loss_2)\n",
    "print('Optimal solution gradient',w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (g): \n",
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART (j): \n",
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section #1.2: Binary Classification\n",
    "\n",
    "Please follow our instructions in the same order to solve the binary classification problem.\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='train')\n",
    "#X_test, y_test = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='t10k')\n",
    "\n",
    "X_train = np.load('./data/binary_classification/X_train.npy')\n",
    "y_train = np.load('./data/binary_classification/y_train.npy')\n",
    "X_test = np.load('./data/binary_classification/X_test.npy')\n",
    "y_test = np.load('./data/binary_classification/y_test.npy')\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PART (a): \n",
    "# To Visualize a point in the dataset\n",
    "index = 11\n",
    "X = np.array(X_train[index], dtype='uint8')\n",
    "X = X.reshape((28, 28))\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "if y_train[index] == 1:\n",
    "    label = 'Dress'\n",
    "else:\n",
    "    label = 'Shirt'\n",
    "print('label is', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron\n",
    "In the following cells, you will build Perceptron Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (b),(c): \n",
    "# Implement the perceptron Algorithm and compute the number of mis-classified point\n",
    "N = X_train.shape[0] # Number of data point train\n",
    "N_test = X_test.shape[0] # Number of data point test\n",
    "d = X_train.shape[1] # Number of features\n",
    "loss_hist = []\n",
    "W = np.zeros((d+1,1))\n",
    "X_train_h = np.hstack((np.ones((N,1)), X_train))\n",
    "X_test_h = np.hstack((np.ones((N_test,1)), X_test))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test accuracy in the same plot\n",
    "# for m range from 1 to N\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression\n",
    "In the following cells, you will build a logistic regression. You will implement its loss function, then subsequently train it with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.Logistic import Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (f): \n",
    "X_train = np.load('./data/binary_classification/X_train.npy')\n",
    "y_train = np.load('./data/binary_classification/y_train.npy')\n",
    "X_test = np.load('./data/binary_classification/X_test.npy')\n",
    "y_test = np.load('./data/binary_classification/y_test.npy')\n",
    "## Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "N,d = X_train.shape\n",
    "logistic = Logistic(d=d, reg_param=0)\n",
    "loss, grad = logistic.loss_and_grad(X_train,y_train)\n",
    "print('Loss function=',loss)\n",
    "print(np.linalg.norm(grad,ord=2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (g):\n",
    "## Complete train_LR function in Logisitc.py file\n",
    "loss_history, w = logistic.train_LR(X_train,y_train, eta=1e-6,batch_size=100, num_iters=5000)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_loss_hist.pdf')\n",
    "print('Weight squared norm',np.linalg.norm(w,ord=2)**2)\n",
    "print('Final loss',loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (h)\n",
    "# Complete predict function in Logisitc.py file and compute the percentage of mis-classified points\n",
    "y_pred = logistic.predict(X_test)\n",
    "test_err = np.sum((y_test!=y_pred))*100/X_test.shape[0]\n",
    "print(test_err,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (i): \n",
    "Batch = [1, 50 , 100, 200, 300]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the Logistic regression for different batch size. Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(Batch,test_err)\n",
    "plt.xlabel('Batch_size')\n",
    "plt.ylabel('Test_error')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_Batch_test.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM\n",
    "In the following cells, you will build SVM. You will implement its loss function, then subsequently train it with mini-batch gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.SVM import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (l): \n",
    "## Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "N,d = X_train.shape\n",
    "svm = SVM(d=d, reg_param=0)\n",
    "loss, grad = svm.loss_and_grad(X_train,y_train)\n",
    "print('Loss function=',loss)\n",
    "print(np.linalg.norm(grad,ord=2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (m): \n",
    "## Complete train_svm function in SVM.py file \n",
    "loss_history, w = svm.train_svm(X_train,y_train, eta=1e-6,batch_size=50, num_iters=5000)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('./plots/svm_loss_hist.pdf')\n",
    "print(np.linalg.norm(w,ord=2)**2)\n",
    "print(loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (n): \n",
    "# Complete predict function in SVM.py file and compute the percentage of mis-classified points\n",
    "y_pred = svm.predict(X_test)\n",
    "test_err_prec = np.sum((y_test!=y_pred))*100/X_test.shape[0]\n",
    "print(test_err_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (o) \n",
    "Batch = [1, 50 , 100,200, 300]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the SVM for different batch size Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(Batch,test_err)\n",
    "plt.xlabel('Batch_size')\n",
    "plt.ylabel('Test_error')\n",
    "plt.show()\n",
    "fig.savefig('./plots/svm_Batch_test.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section #1.3: Multi-Class Logistic Regression and Adaboost\n",
    "\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('./data/fashion-mnist', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('./data/fashion-mnist', kind='t10k')\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)\n",
    "label = {0:'T-shirt/Top',1:'Trouser',2:'Pullover',3:'Dress',4:'Coat',5:'Sandal',6:'Shirt',7:'Sneaker',8:'Bag',9:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (a) \n",
    "# To Visualize a point in the dataset\n",
    "index = 11\n",
    "X = np.array(X_train[index], dtype='uint8')\n",
    "X = X.reshape((28, 28))\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "fig.savefig('./plots/Sample.pdf')\n",
    "print('label is', label[y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi-Class Logistic Regression\n",
    "\n",
    "In the following cells, you will build a Multi-Class logistic regression. You will implement its loss function, then subsequently train it with gradient descent. You will implement L1 norm regularization, and choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.MLogistic import MLogistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (d)\n",
    "## Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "logistic = MLogistic(dim=[num_classes,num_features], reg_param=0)\n",
    "loss, grad = logistic.loss_and_grad(X_train[:5000],y_train[:5000])\n",
    "print('Loss function=',loss)\n",
    "print('Frobenius norm of grad=',np.linalg.norm(grad))\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (e): \n",
    "## Complete train_LR function in Logistic.py file \n",
    "loss_history, w = logistic.train_LR(X_train,y_train, eta=1e-7,batch_size=200, num_iters=1500)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('./plots/loss_hist.pdf')\n",
    "print(np.linalg.norm(w))\n",
    "print(loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (f): \n",
    "## Complete predict function in Logistic.py file and compute the trainin error and the test error\n",
    "\n",
    "y_train_pred = logistic.predict(X_train)\n",
    "print('training error:', 1-np.mean(np.equal(y_train,y_train_pred)))\n",
    "y_test_pred = logistic.predict(X_test)\n",
    "print('test error:',1-np.mean(np.equal(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (h): \n",
    "reg = [0,1e-6,1e-3,1e-2,1e-1,1]\n",
    "train_err =np.zeros((len(reg),1))\n",
    "test_err =np.zeros((len(reg),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(train_err)\n",
    "plt.plot(test_err, color='black')\n",
    "plt.show()\n",
    "fig.savefig('./plots/Regularization.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (i): \n",
    "T = 200\n",
    "N = X_train.shape[0]\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "train_err = np.zeros((T,1))\n",
    "test_err = np.zeros((T,1))\n",
    "\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# as a function of number of classifiers T for Adaboost Algorithm. \n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(train_err)\n",
    "plt.plot(test_err, color='black')\n",
    "plt.show()\n",
    "fig.savefig('./plots/Adaboost.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
